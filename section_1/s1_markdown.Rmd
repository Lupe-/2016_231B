---
title: "231B Section 1"
author: "Guadalupe Tuñón \\ https://github.com/Lupe-/2016_231B"
date: "January 21, 2015"
output:
  slidy_presentation:
      fig_height: 5
      font_adjustment: +1
---

```{r, echo=FALSE}
# libraries
library(png)
library(grid)
library(ggplot2)
# wd
setwd("~/Dropbox/231B_Spring2015/231b_section_materials_2015/section_1")
```


Today
=====================================================================
1. Overview
2. Simulations in R: the birthday problem 
3. Sampling simulations (one sample)
4. Functions in R: the difference of two means


Sections
=====================================================================

## Two goals:
- Illustrating and understanding the concepts covered in lecture
- Going from learning R to programming in R


For some sections, we will be using codeshare: http://www.codeshare.io/NQ7Gn

> We will be writing simulations and functions at least 80% of our time, it will be important to get familiar with this!

Some material to cover the basics:

* Functions: [General thought on what makes a good function](http://nicercode.github.io/guides/functions/), [beginner](http://nicercode.github.io/intro/writing-functions.html), [intermediate/advanced](http://adv-r.had.co.nz/Functions.html). Shalizi on functions, [part 1](http://www.stat.cmu.edu/~cshalizi/statcomp/14/lectures/06/lecture-06.pdf) & [part 2](http://www.stat.cmu.edu/~cshalizi/statcomp/14/lectures/07/lecture-07.pdf).
* Simulations: [Repeating things](http://nicercode.github.io/guides/repeating-things/), [flow control and looping](http://www.stat.cmu.edu/~cshalizi/statcomp/14/lectures/03/lecture-03.pdf).


Problem Sets
=====================================================================
- Send a digital copy to guadalupe.tunon+231b@gmail.com
- *Compiled versions* of your code. ".R" files will not be considered for grading. 
- Use [sweave, knitr](https://support.rstudio.com/hc/en-us/articles/200552056-Using-Sweave-and-knitr) or [R markdown](http://rmarkdown.rstudio.com/pdf_document_format.html)
- When debugging your code, [you might find Cosma Shalizi's advice helpful](http://www.stat.cmu.edu/~cshalizi/statcomp/14/lectures/13/lecture-13.pdf)
- Practice [defensive programming](http://software-carpentry.org/v5/novice/python/05-defensive.html)
- Problem sets due at 12 mignight on Wednesdays


----


Why program? (Falster)
=====================================================================

* Scripting allows you to automate analyses, and therefore scale-up without a big increase in effort

* Easier to pick up a project and start working on it again

* Chunks of code can also be reused in new projects

* Allows for effective collaboration with people 

Nice R code: code that is easy to read, easy to write, runs fast, gives reliable results, is easy to reuse in new projects, and is easy to share with collaborators

[Six steps to a better relationship with your future self.](http://polmeth.wustl.edu/methodologist/tpm_v18_n2.pdf)



Why program? (Shalizi)
=====================================================================

- **Independence**: Otherwise, you rely on someone else having given you exactly the right tool

- **Honesty**: Otherwise, you end up distorting your problem to match the tools you have

- **Clarity**: Making your method something a machine can do disciplines your thinking and makes it public


> Programs should be written for people to read, and only incidentally for
machines to execute (Abelson and Sussman)


Programming is writing functions to transform inputs into outputs (Shalizi)
=====================================================================
- Good programming ensures the transformation is done easily and correctly

- Machines are made out of machines; functions are made out of functions, like $f(a,b) = a^2 + b^2$

- The route to good programming is to take the big transformation and break it down into smaller ones, and then break those down, until you come to tasks which the built-in functions can do


2. The birthday problem (Feller 1957) 
=====================================================================
Suppose we sample 25 people from a large population and put them in a room. What is the probability that two or more of them have the same birthday?

`X = number of birthday matches` 

### Simplifying assumptions:
- Drawing from a *very* large population
- Birthdays are uniformly distributed throughout the year
- Ignore leap years and pretend there are only 365 possible birthdays

Is $X$ a random variable? If so, what is the box model?


----

We could solve this problem using basic probability, but we will instead write a computer simulation. How can we solve this problem with R?

How would we use R to draw a random sample of $25$ people from this box?

----

Simulating birthdays for $25$ people
```{r}
birthdays <- sample(1:365, 25, replace=T)
birthdays
```

----

Finding the number of birthday matches among $25$ people
```{r}
X <- 25 - length(unique(birthdays))
```
Here we have simulated the situation for one room with $25$ people, where there is/are
`r X` shared birthday(s)

----

If we could repeat this process a very large number of times, we would obtain many realizations of the random variable $X$, and thus a good idea of its distribution.

How could we do this in R?

----

We can use a loop to simulate $X$ for many rooms with $25$ people
```{r}

X <- NA # Creating placeholder vector
r <- 10000 # We will simulate 10,000 rooms

for (i in 1:r){ 
  
  birthdays <- sample(1:365, 25, replace=T)
  X[i] <- 25 - length(unique(birthdays))
  
}
```

----

```{r}
qplot(X, binwidth=.5)
```

-----------------

How can we use our 10,000 realizations of $X$ to get the approximate probability of no match $P(X = 0)$ or match $P(X>0)$?

What about $E(X)$?

----

We can count the proportion of rooms with $X=0$
```{r}
mean(X!=0)
```

This is very close to the combinatorial solution which is 0.5687

----

We can take the average of the realizations of $X$ to get a good approximation to $E(X)$.
```{r}
mean(X) 
```
This result is not easily obtained by combinatorial methods.


3a. Sample mean as an unbiased estimator of the population mean
======================================================================

First we will need to "create" a population, a box of tickets

```{r}
population <- c(4,5,7,12,7,8,9,-3,5,8,9,3,2,3,4,6,10,4,6,7,8,9,2)

N <- length(population) # number of observations in the population
N

pop_mean <- mean(population) # population mean
pop_mean 

pop_sd <- sd(population) # population standard deviation
pop_sd
```

----

We will draw several random samples of 8 observation without replacement 
```{r}
s1 <- sample(population, size=8, replace = FALSE)

s2 <- sample(population, size=8, replace = FALSE)

s3 <- sample(population, size=8, replace = FALSE)

s4 <- sample(population, size=8, replace = FALSE)

samples <- rbind(s1, s2, s3, s4)

samples
```

----

Remember the population mean: `r pop_mean`

And the means of the samples 

```{r} 
apply(samples, MARGIN=1, FUN=mean) 
```

By chance each given sample mean may be a little higher or lower than the population mean. 

How can we use R to show that the sample mean is an unbiased estimator of the population mean?

----

For this, we will use a simulation. We will repeat the sample process 10,000 times.

```{r}
sample_mean <- NA

for (i in 1:10000){
  
  sample <- sample(population, size=8, replace = FALSE)
  sample_mean[i] <- mean(sample)
  
}
```

----

```{r}
par(mfrow=c(1,1))
plot(density(sample_mean), col="blue", lwd=3,
     main="Distribution of sample means")
abline(v=pop_mean, col="red", lwd=2)

```

3b. Variance of the sample mean as m gets closer to N.
======================================================================

So far, $m=8$. We now need a new simulation that adds a new step: we need to vary the size of m.

----

```{r, eval=FALSE}

rep <- 10000

for (m in 9:20){
  sample_mean <- NA
  
  for (i in 1:rep){
    sample <- sample(population, size=m, replace = FALSE)
    sample_mean[i] <- mean(sample)
  }
  
  lines(density(sample_mean), lwd=3,
        col=paste0("grey",140-(7*m)))
}

```

What do we expect? Why?

----

```{r, echo=FALSE}

plot(density(sample_mean), col="blue", ylim=c(0,1.6),
     main="Distribution of sample means", lwd=3)
abline(v=pop_mean, col="red", lwd=3)

rep <- 10000

for (m in 9:20){
  sample_mean <- NA
  
  for (i in 1:rep){
    sample <- sample(population, size=m, replace = FALSE)
    sample_mean[i] <- mean(sample)
  }
  
  lines(density(sample_mean), lwd=3,
        col=paste0("grey",140-(7*m)))
}

```


4. Function for the difference of two means
======================================================================

```{r}
diff_means <- function(y, x){ 
  
  # Calculating difference in means
  mean1 <- mean(y[x==1], na.rm=T)
  mean0 <- mean(y[x==0], na.rm=T)
  diff <- mean1 - mean0
  
  # Calculating number of observations
  N <- length(na.omit(y))
  
  # Preparing output
  res <- c(mean1, mean0, diff, N)
  names(res) <- c("Mean 1", "Mean 0", "Difference", "N")
  
  return(c(res))
}
```

----

A little silly, but let's do this

```{r}
print(diff_means)
```

----

To try our function, we will use the small dataset in Gerber & Green (2012)
```{r}
gg_data <- as.data.frame(cbind(c(10,15,20,20,10,15,15), 
                               c(15,15,30,15,20,15,30)))
names(gg_data) <- c("Y_i0", "Y_i1")
save(gg_data, file="gg_data.Rda")
```

(`"gg_data.Rda"` uploaded on bcourses)

----

We will need to "create" a treatment vector...
```{r}
gg_data$treat <- sample(c(0,1), 7, replace=T)
gg_data$treat
```

...and a column with the "observed" outcomes
```{r}
gg_data$observed <- ifelse(gg_data$treat==1, gg_data$Y_i1, gg_data$Y_i0)
```

----

```{r}
with(gg_data, diff_means(observed, treat))

mean(gg_data$Y_i1[gg_data$treat==1])
mean(gg_data$Y_i0[gg_data$treat==0])

```

How could we use this to formally test our code?

----

```{r}

diff <- mean(gg_data$Y_i1[gg_data$treat==1]) - mean(gg_data$Y_i0[gg_data$treat==0])
  
stopifnot(diff==with(gg_data, diff_means(observed, treat))[3])

```

----

Problem set 1
==============================================

- Simulation to show the difference of two means is unbiased
- Extend the difference of means function to calculate the SE of the difference (we will cover SEs in lecture on Tuesday)
- Replicate figures and tables in Dunning and Harrison (2010) -- you will need your new function for this!

